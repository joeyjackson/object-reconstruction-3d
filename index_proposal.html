<!DOCTYPE html>
<html lang="en"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Class Project
  | Georgia Tech | Fall 2018: CS 4476</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>

<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name -->
<h1>3D Object Reconstruction</h1>
<span style="font-size: 20px; line-height: 1.5em;"><strong>
Joey Jackson - ejackson61<br>
Corey Zheng - czheng45<br>
Matthew Fraschilla - mfraschilla3<br>
Chris O’Brien - cobrien42
</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4476 Intro to Computer Vision: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h3>1. Problem Statement</h3>

The goal of the project is to create a 3D computer model of an object given multiple images of the object from known world positions. The user will provide an object and place it inside of a controlled rig, which will allow the camera to take pictures of the object from known positions. The output will then be a set of 3D points and faces or edges that represent the object the user provided.
<br><br>
<!-- figure -->
<h3>2. Approach</h3>
The software system will be given a set of images of an object along with the corresponding vectors for the world position and orientation of the camera as the picture was taken. These positions and orientations will be controlled and collected by the use of a rig, which will hold the camera at a fixed distance and angle and allow the camera to revolve about an axis at this distance. The rig will be surrounded by a white enclosure without any texture to provide a consistent background. To use the system, the user will place an object inside the rig, centered about the rotation axis, and then take a set of images at each of the predefined positions.
<br><br>
The software will take in this set of images, position vectors, and orientation vectors. For pairs of images, the system will perform feature detection and matching between the images to find correspondences. The system will then use stereo matching with the correspondences and camera position data to find the 3D world positions of each of the features. After a point cloud of the object is created, our system will then perform some post-processing on the point cloud using edge detection algorithms on the original images to determine which of the points in the set are connected to form edges and faces in the object.
<br><br>
The output will then be a set of 3D points and faces or edges that represent the object the user provided. This set of points and faces can then possibly be rendered using graphics software or a game engine.

<br><br>
<!-- Approach -->
<h3>3. Experiments and Results</h3>
<h4>3.1 System setup</h4>
There are 4 main steps that will need to be created for the system to be operational.
<ol>
<li>Create the camera rig and perform image collection of an object</li>
<li>Perform feature detection and matching</li>
<li> stereo matching to determine 3D world positions of features</li>
<li>Run an edge detection algorithm to determine edges between feature points</li>
</ol>
<h4>3.2 Implementation</h4>
In order to complete this project, we will implement some of the functionality ourselves and use functionality from pre-existing code for other parts. For many of the well established algorithms, we will use functions from the OpenCV library. We will use it for feature detection, feature matching, and edge detection algorithms. The parts of the project that we will implement ourselves are building the camera rig and performing image collection of objects, performing stereo matching of corresponding features given camera world positions and orientation to determine 3D world positions of features, and an algorithm to determine relevant edges between features from the point set to create an edge set for the object.

<h4>3.3 Data Usage and Collection</h4>
We plan to collect our own data for most of this project. We will do this by building a camera rig to be able to accurately move our camera around an object so we know precisely the location and orientation that each image was taken from. The more accurate we can know the camera position, the better the system will perform.
The assumptions for data collection are as follows:
<ul>
<li>The rig is encapsulated by a smooth white background.</li>
<li>Lighting is consistent.</li>
<li>The rig does not produce shadows in field of view of the camera.</li>
</ul>

The data collection process for any given object will be as follows:
<ol>
<li>Place object within rig such that it is fully visible at all possible camera positions.</li>
<li>Take a picture and rotate the arm holding the camera by 10 degrees. Repeat this for 3 full revolutions.</li>
</ol>

The data acquired will not need any labeling or processing. It will be used directly for feature extraction and edge detection. Control experiments used to test system components as described in 3.4 will involve the use of grid paper and a cube. These will have few strictly designated features, such as printed points on the paper and the vertices of the cube, which can be identified manually.

<h4>3.4 Planned Experiments</h4>
To evaluate system performance during development, we will run a number of experiments to test individual components of the system before characterizing our system as a whole.
<br><br>
The component tests should reveal information about uncertain system characteristics such as:
<ul>
<li>Ideal feature scale for detection</li>
<li>Effect of image angle on object</li>
<li>The limits of geometry types that can be properly extracted</li>
<li>Influence of colors and textures on detected features and edges</li>
</ul>
We predict that the experiments will show that the approach difficulty will increase with more complex geometry and confounding factors which make relevant edges and vertices harder to detect. For example, drastic changes in color on a planar surface may cause false detection of vertices, while curved surfaces be difficult to reconstruct due to their lack of vertices.

<h5>3.4.1 Feature extraction & Edge detection</h5>
The first component of the system to test will be feature extraction and edge detection. Tests will be performed in graduating levels, and will be done to characterize system parameters.
<br><br>
Initial tests will use grid paper and a simple 3D cube, using images taken from different angles and distances against a white background to determine the best distance and angle for the camera rig. The criteria to determine ‘best’ for feature detection will be the counting of correctly matched features (points/vertices) as well as spurious or dropped features. Edge criteria will similarly be determined by the counting of correctly matched edges and spurious or dropped edges.
<br><br>
Afterwards, tests on more complicated geometries and surfaces will be performed to determine the extent of object variety the approach will work on. Possible categories include:
<ul>
<li>Rectangular prism (ex: Rubik’s cube)</li>
<li>Round (ex: ping-pong ball)</li>
<li>Cylindrical (ex: pen)</li>
<li>Concave facets/cavities (ex: cup)</li>
<li>Varying surface reflection, absorption, and transparency (ex: water bottle)</li>
<li>Varying colors and textures</li>
<li>Objects of varying scale</li>
</ul>

<h5>3.4.2 Stereo matching</h5>
The second component of the system will be testing stereo matching and its ability to determine the 3D coordinate of a given feature. Similar to section 3.4.1, tests will be performed in graduating levels.
<br><br>
Initial tests will use extracted features on grid paper to determine the accuracy of point detection in 3D space, and consistency over multiple image pairs at known positions. Specifically, the errors in positions of determined points relative to each other will be quantified as a measure of accuracy due to the difficulty of measuring the actual position of the object in a defined global frame in each experiment.
<br><br>
The tests will progress from using grid paper to a 3D cube, and then to more complicated geometries as described in section 3.4.1.
<br><br>
Finally, we will test the entire system using various objects. As in the other experimental sections, the objects will gradually increase in complexity, starting from grid paper and a cube. If the system works well with those objects, we will test it on objects with more complex geometry or rounded surfaces such as a water bottle or coffee mug.

<h4>3.5 Success Criteria</h4>
The criteria for basic success is as follows:
The system will first be used to model simple objects with few, known feature points: grid paper and a cube. The generated point cloud will then be compared to the known dimensions and geometry of the objects. Error between corresponding points will be calculated and summed, and if the total error of generated points is below some threshold, the system will be characterized as successful.
<br><br>
The criteria for advanced success is as follows:
The system will be used to model a more complicated object that has already been modeled; for example, a possible object could be a commercial stepper motor which has a 3D model provided by the manufacturer. The data generated by the system will then be compared to the known model of the object using metrics such as: dropped or spurious vertices, dimensional tolerance between two features, and a qualitative comparison between our point cloud rendered in 3D space and the provided model.

  <hr>
  <footer>
  <p>Joey Jackson, Corey Zheng, Matthew Fraschilla, Chris O'Brien</p>
  </footer>
</div>
</div>

<br><br>

</body></html>